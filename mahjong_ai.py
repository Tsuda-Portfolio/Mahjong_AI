# -*- coding: utf-8 -*-
"""
mahjong_ai_transformer.py  
==========================
TensorFlow å®Ÿè£…ã‚’ **PyTorch + Deep Transformer** ã«ç·ç½®æ›ã€‚

* **é–¢æ•°åã¯æ—§ç‰ˆ (make_model / train_ai / run_ai / test_ai / get_ai_dahai) ã®ã¾ã¾**
* 40,000 è¡Œã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã§ã‚‚éå­¦ç¿’ã‚’é¿ã‘ã‚‹æ·±å±¤ãƒãƒƒãƒˆ (8 å±¤ Transformer + Dropout + WeightDecay)
* `--resume` ã§é€”ä¸­å†é–‹å¯ã€‚å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«å®Œå…¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜

ãƒ‡ãƒ¼ã‚¿å½¢å¼
-----------
CSV / TXT 1 è¡Œ 1 ã‚µãƒ³ãƒ—ãƒ«ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯
```
14æšç‰Œæ–‡å­—åˆ— æ‰“ç‰Œç‰Œæ–‡å­—åˆ—
ä¾‹) 1m1m1m2m3m6m7m5p6p7p8s9s9s9s 8s
```

ä¾å­˜
----
```
pip install torch torchmetrics tqdm
```

"""
import argparse
from pathlib import Path
from typing import List, Tuple, Any, Dict
import time
import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchmetrics.classification import MulticlassAccuracy
from tqdm.auto import tqdm

import mahjong_common as mjc
import mahjong_loader as mjl

import torch
import torch.nn.functional as F
from torch.distributions import Categorical

# ----------------- å®šæ•° -----------------
NUM_HAI = 34          # ç‰Œç¨®
TOKEN_PAD = 34        # pad ãƒˆãƒ¼ã‚¯ãƒ³ id
MAX_HAND = 14         # ç‰Œæšæ•° (å¸¸ã« 14)
CKPT_DIR = Path("ckpt")
CKPT_DIR.mkdir(exist_ok=True)

# ----------------- ç‰Œï¼œï¼ID å¤‰æ› -----------------
_suit_map = {'m': 0, 'p': 9, 's': 18, 'z': 27}
_id_map_cache: Dict[str, int] = {}

# æ¼¢å­—ç‰Œã®ç½®æ›ãƒ†ãƒ¼ãƒ–ãƒ«
zhai_map = {"æ±": "1z", "å—": "2z", "è¥¿": "3z", "åŒ—": "4z", "ç™½": "5z", "ç™¼": "6z", "ç™º": "6z", "ä¸­": "7z"}

def hai_str_to_id(hai: str) -> int:
    if hai in _id_map_cache:
        return _id_map_cache[hai]
    rank, suit = hai[0], hai[1]
    base = _suit_map[suit]
    _id_map_cache[hai] = base + int(rank) - 1
    return _id_map_cache[hai]

import random  # â† å¿…è¦ãªã‚‰ã‚¯ãƒ©ã‚¹å¤–ã§è¿½åŠ ã—ã¦ãŠã„ã¦OK

class MahjongDataset(Dataset):
    def __init__(self, path: Path):
        self.samples: List[Tuple[List[int], int]] = []
        with path.open(encoding="utf-8") as f:
            for ln in f:
                ln = ln.strip()
                if not ln:
                    continue
                for k, v in zhai_map.items():
                    ln = ln.replace(k, v)
                try:
                    hand_str, out_str = ln.split()
                    if len(hand_str) != 28 or len(out_str) != 2:
                        continue
                    tiles = [hand_str[i:i + 2] for i in range(0, 28, 2)]
                    random.shuffle(tiles)  # â† â­ã“ã“ï¼

                    if any(len(t) != 2 for t in tiles):
                        continue
                    ids = [hai_str_to_id(t) for t in tiles]
                    if len(ids) != 14:
                        continue
                    dahai = hai_str_to_id(out_str)
                    self.samples.append((ids, dahai))
                except Exception as e:
                    print(f"âš ï¸ è¡Œå‡¦ç†ä¸­ã«ä¾‹å¤–: {ln} â†’ {e}")


    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int):
        ids, label = self.samples[idx]
        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)


# ----------------- ãƒ¢ãƒ‡ãƒ« -----------------
class MahjongTransformer(nn.Module):
    def __init__(self, d_model: int = 256, n_head: int = 8, num_layers: int = 8,
                 dim_feedforward: int = 1024, dropout: float = 0.15):
        super().__init__()
        self.embed = nn.Embedding(NUM_HAI + 1, d_model)  # + PAD
        self.pos_emb = nn.Parameter(torch.randn(MAX_HAND, d_model))
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_head,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True,
            activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, NUM_HAI)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B,14) long
        mask = x == TOKEN_PAD
        h = self.embed(x) + self.pos_emb  # (B,14,D)
        h = self.encoder(h, src_key_padding_mask=mask)
        h = self.norm(h)
        h = h.mean(dim=1)  # (B,D)
        return self.head(h)  # (B,34)

# ----------------- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° -----------------
model: MahjongTransformer | None = None
optimizer: AdamW | None = None
scheduler: CosineAnnealingLR | None = None
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
acc_metric = MulticlassAccuracy(num_classes=NUM_HAI, average="micro").to(device)

# ----------------- æ—§é–¢æ•°å API -----------------

def make_model() -> None:
    """ãƒ¢ãƒ‡ãƒ«ãƒ»optimizerãƒ»scheduler ã‚’åˆæœŸåŒ–ã—ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨­å®š"""
    global model, optimizer, scheduler
    model = MahjongTransformer().to(device)
    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)
    scheduler = CosineAnnealingLR(optimizer, T_max=100)  # ä»®ã€‚train_ai() å†…ã§ set

# --------- å†…éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---------

def _save_ckpt(epoch: int, path: Path):
    torch.save({
        "epoch": epoch,
        "model": model.state_dict(),
        "opt": optimizer.state_dict(),
        "sched": scheduler.state_dict(),
    }, path)


def _load_ckpt(path: Path) -> int:
    ckpt = torch.load(path, map_location=device)
    model.load_state_dict(ckpt["model"])
    optimizer.load_state_dict(ckpt["opt"])
    scheduler.load_state_dict(ckpt["sched"])
    return ckpt["epoch"]

# --------- å­¦ç¿’ ---------

def train_ai(filename: str, num_epochs: int):
    path = Path(filename)
    assert path.exists(), f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}"

    full_ds = MahjongDataset(path)
    val_size = max(1, int(0.1 * len(full_ds)))
    train_size = len(full_ds) - val_size
    train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])

    train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)

    scheduler.T_max = 1000  # é©å½“ã«å¤§ããã—ã¦ãŠãï¼ˆå‘¨æœŸåˆ¶å¾¡ï¼‰

    criterion = nn.CrossEntropyLoss()
    ckpt_path = CKPT_DIR / "mahjong_transformer.pt"

    start_epoch = 0
    if ckpt_path.exists():
        start_epoch = _load_ckpt(ckpt_path)
        print(f"[Resume] {start_epoch} ã‚¨ãƒãƒƒã‚¯ç›®ã‹ã‚‰å†é–‹â€¦")

    for epoch in range(start_epoch + 1, start_epoch + num_epochs + 1):  # â† ä¿®æ­£ï¼
        model.train()
        total_loss = 0.0
        for xb, yb in tqdm(train_loader, desc=f"Epoch {epoch} - train", leave=True):
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad(set_to_none=True)
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * yb.size(0)
        scheduler.step()
        train_loss = total_loss / len(train_loader.dataset)

        # validation
        model.eval()
        acc_metric.reset()
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                preds = model(xb).argmax(1)
                acc_metric.update(preds, yb)
        val_acc = acc_metric.compute().item()
        print(f"Epoch {epoch:3d}: loss {train_loss:.4f}, val acc {val_acc:.4%}")

        _save_ckpt(epoch, ckpt_path)

    print(f"[å®Œäº†] å­¦ç¿’çµ‚äº†ã€‚è¿½åŠ ã§ {num_epochs} ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã—ã¾ã—ãŸ")


from tqdm import tqdm

def reinforce_train(num_games: int = 100, lambda_pg: float = 1.0):
    """
    run_ai() ã®ã‚ˆã†ã«ãƒ—ãƒ¬ã‚¤ã—ãªãŒã‚‰ã€REINFORCE ã«ã‚ˆã£ã¦ fine-tuningã€‚
    `lambda_pg` ã¯ policy gradient loss ã®é‡ã¿ã€‚
    """
    model.train()
    optimizer.zero_grad(set_to_none=True)

    log_probs = []
    rewards = []
    win_count = 0
    tenpai_count = 0

    for i in tqdm(range(num_games), desc="[å¼·åŒ–å­¦ç¿’] Simulating"):
        mjc.init_yama()
        tehai = mjc.get_haipai()
        tsumo_count = 0
        total_reward = 0.0

        tenpai = False  # â† while ã®å‰ã«ã“ã‚Œã‚’è¿½åŠ 

        # ãƒ«ãƒ¼ãƒ—ä¸­ã«ä¸€åº¦ã§ã‚‚ãƒ†ãƒ³ãƒ‘ã‚¤ã—ãŸã‹ã‚’è¨˜éŒ²
        while tsumo_count < 18:
            tsumo = mjc.get_tsumo()
            if tsumo == -1:
                break
            tsumo_count += 1
            tehai[tsumo] += 1

            if mjc.is_agari(tehai):
                total_reward += 200.0
                win_count += 1
                break

            if not tenpai and mjc.is_tenpai(tehai):
                tenpai = True

            ids = _tehai_vector_to_idlist(tehai)
            x = torch.tensor([ids], dtype=torch.long, device=device)
            logits = model(x)
            probs = F.softmax(logits[0], dim=0)
            probs_masked = probs.clone()
            for i in range(NUM_HAI):
                if tehai[i] == 0:
                    probs_masked[i] = 0.0
            probs_masked /= probs_masked.sum() + 1e-6

            dist = Categorical(probs_masked)
            action = dist.sample()
            log_probs.append(dist.log_prob(action))
            tehai[action.item()] -= 1

        # ======== å ±é…¬è¨ˆç®—ï¼ˆãƒ«ãƒ¼ãƒ—å¾Œï¼‰========
        if total_reward == 0:
            if mjc.is_tenpai(tehai):
                total_reward = 200.0
                tenpai_count += 1
            elif tenpai:
                total_reward = -5000.0  # ãƒ†ãƒ³ãƒ‘ã‚¤ã—ã¦ãŸã®ã«å´©ã‚ŒãŸ
            else:
                total_reward = -100.0   # ä¸€åº¦ã‚‚ãƒ†ãƒ³ãƒ‘ã‚¤ã§ããšçµ‚å±€

        rewards.append(total_reward)

    # normalize rewards
    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)
    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)

    policy_loss = 0.0
    for log_prob, reward in zip(log_probs, rewards):
        policy_loss += -log_prob * reward
    policy_loss /= len(rewards)

    # ğŸ”¥ entropy bonus ã‚’åŠ ãˆã‚‹ï¼ˆå­¦ç¿’ã®åã‚ŠæŠ‘åˆ¶ï¼‰
    entropy = -torch.sum(probs_masked * torch.log(probs_masked + 1e-6))
    policy_loss -= 0.01 * entropy  # â† ã“ã“ãŒè¿½åŠ ç‚¹ï¼

    # cross-entropy lossï¼ˆ1ãƒãƒƒãƒã ã‘ã§OKï¼‰
    path = Path("dahai_data.txt")
    full_ds = MahjongDataset(path)
    loader = DataLoader(full_ds, batch_size=512, shuffle=True, num_workers=2)
    ce_loss_total = 0.0
    criterion = nn.CrossEntropyLoss()

    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        with torch.no_grad():
            logits = model(xb)
            ce_loss = criterion(logits, yb)
            ce_loss_total += ce_loss.item()
        break

    total_loss = ce_loss_total + lambda_pg * policy_loss
    total_loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()

    # çµæœè¡¨ç¤º
    print(f"[å¼·åŒ–å­¦ç¿’] å’Œäº†: {win_count} / {num_games}, è´ç‰Œ: {tenpai_count}")
    print(f"[å¼·åŒ–å­¦ç¿’] CE loss: {ce_loss_total:.4f}, PG loss: {policy_loss.item():.4f}, total: {total_loss.item():.4f}")



# --------- æ¨è«– ---------

def _tehai_vector_to_idlist(tehai: List[int]) -> List[int]:
    """34 é•·ã®æšæ•°ãƒ™ã‚¯ãƒˆãƒ« â†’ 14 å€‹ã® id list"""
    ids = []
    for idx, cnt in enumerate(tehai):
        ids.extend([idx] * cnt)
    assert len(ids) == 14, "æ‰‹ç‰Œæšæ•°ãŒ 14 æšã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    return ids


def get_ai_dahai(ai_in: List[int], ai_out: torch.Tensor) -> int:
    """ai_out (34 logits) ã«åŸºã¥ãã€ai_in ã«å­˜åœ¨ã™ã‚‹ç‰Œã§æœ€å¤§ç¢ºç‡ã®ã‚‚ã®ã‚’è¿”ã™"""
    scores = ai_out.clone()
    while True:
        idx = torch.argmax(scores).item()
        if ai_in[idx] > 0:
            return idx  # æ‰“ç‰Œæ±ºå®š
        scores[idx] = -math.inf  # ãã®ç‰ŒãŒç„¡ã„ã®ã§æ¬¡ç‚¹ã¸


def test_ai():
    """1 å±€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦çµæœã‚’è¿”ã™"""
    mjc.init_yama()
    tehai = mjc.get_haipai()
    tsumo_count = 0
    tenpai = False
    
    while tsumo_count < 18:
        tsumo = mjc.get_tsumo()
        if tsumo == -1:
            break
        tsumo_count += 1
        tehai[tsumo] += 1
        if mjc.is_agari(tehai):
            tehai[tsumo] -= 1
            info = mjc.AgariInfo(tehai, tsumo)
            return info.get_point(), info.get_yaku_strings(), False

        ids = _tehai_vector_to_idlist(tehai)
        logits = model(torch.tensor([ids], dtype=torch.long, device=device))
        dahai = get_ai_dahai(tehai, logits[0])
        tehai[dahai] -= 1
        if not tenpai and mjc.is_tenpai(tehai):
            tenpai = True

    if mjc.is_tenpai(tehai):
        return 0, [], False
    return -1, [], tenpai


def train_loop_alternating(
    datafile: str = "dahai_data.txt",
    alt_count: int = 10,
    reinforce_games: int = 100,
    epochs_each: int = 0,
):
    """
    å¼·åŒ–å­¦ç¿’ã¨æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’äº¤äº’ã«ç¹°ã‚Šè¿”ã™ãƒ«ãƒ¼ãƒ—
    datafile       : æ•™å¸«ã‚ã‚Šãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
    alt_count      : å¼·åŒ–å­¦ç¿’ â‡„ æ•™å¸«ã‚ã‚Šå­¦ç¿’ ã®äº¤äº’å›æ•°
    reinforce_games: 1å›ã‚ãŸã‚Šã®å¼·åŒ–å­¦ç¿’ã®å±€æ•°
    epochs_each    : 1å›ã‚ãŸã‚Šã®æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°
    """
    make_model()
    ckpt = CKPT_DIR / "mahjong_transformer.pt"
    if ckpt.exists():
        _load_ckpt(ckpt)

    for i in range(1, alt_count + 1):
        print(f"\nğŸ§  === Step {i}: å¼·åŒ–å­¦ç¿’ {reinforce_games}å±€ ===")
        reinforce_train(num_games=reinforce_games)

        print(f"\nğŸ“š === Step {i}: æ•™å¸«ã‚ã‚Šå­¦ç¿’ {epochs_each}ã‚¨ãƒãƒƒã‚¯ ===")
        train_ai(datafile, num_epochs=epochs_each)

    print("\nâœ… å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Œäº†")


# æœ€å¾Œã® __main__ éƒ¨åˆ†ã®ã¿å®Œæˆç‰ˆã¨ã—ã¦è¨˜è¼‰
# ä¸Šéƒ¨ã¯æ—¢å­˜ã‚³ãƒ¼ãƒ‰ãã®ã¾ã¾ã§OKï¼ˆreinforce_loop_multiple ã®è¿½åŠ ã‚‚å«ã‚€ï¼‰

import time  # ğŸ”½ ä¼‘æ†©ç”¨ sleep ã‚’ä½¿ã†ãŸã‚ã«å¿…è¦

# ğŸ”½ å¼·åŒ–å­¦ç¿’ã‚’è¤‡æ•°å›ç¹°ã‚Šè¿”ã™é–¢æ•°

def reinforce_loop_multiple(times: int = 5, games_each: int = 100, sleep_sec: int = 30, lambda_pg: float = 1.0):
    """
    å¼·åŒ–å­¦ç¿’ã‚’ times å›ç¹°ã‚Šè¿”ã—ã€å„å›ã”ã¨ã« sleep_sec ç§’ä¼‘æ†©ã™ã‚‹ã€‚
    lambda_pg ã¯ policy gradient loss ã®é‡ã¿ã€‚
    """
    make_model()
    ckpt = CKPT_DIR / "mahjong_transformer.pt"
    if ckpt.exists():
        _load_ckpt(ckpt)

    for i in range(times):
        print(f"\n\U0001f9e0 === å¼·åŒ–å­¦ç¿’ {i+1}/{times} å›ç›® ({games_each} å±€, \u03bb={lambda_pg}) ===")
        reinforce_train(num_games=games_each, lambda_pg=lambda_pg)
        if i < times - 1:
            print(f"\u23f8 æ¬¡ã®å¼·åŒ–å­¦ç¿’ã¾ã§ {sleep_sec} ç§’ä¼‘æ†©...")
            time.sleep(sleep_sec)

    print(f"\n\u2705 å…¨ {times} å›ã®å¼·åŒ–å­¦ç¿’å®Œäº†ï¼")


def run_ai():
    """AI ã‚’ 10,000 å±€ãƒ†ã‚¹ãƒˆã—çµ±è¨ˆå‡ºåŠ›"""
    test_count = 100
    agari = tenpai_ru = tenpai_kuzu = total_pts = 0
    yaku_count: Dict[str, int] = {}

    for _ in tqdm(range(test_count), desc="Sim", leave=False):
        pts, yakus, tenpai_kuzusi = test_ai()
        if pts > 0:
            agari += 1
            total_pts += pts
            for y in yakus:
                yaku_count[y] = yaku_count.get(y, 0) + 1
        elif pts == 0:
            tenpai_ru += 1
        elif tenpai_kuzusi:
            tenpai_kuzu += 1

    print("å’Œäº†:", agari)
    if agari:
        print("å¹³å‡å¾—ç‚¹:", total_pts / agari)
        print("å½¹å†…è¨³:", yaku_count)
    print("æµå±€æ™‚è´ç‰Œ:", tenpai_ru)
    print("è´ç‰Œå´©ã—:", tenpai_kuzu)

# mahjong_common.py ã«è¿½è¨˜
def hai_str_to_id(hai: str) -> int:
    suit_map = {'m': 0, 'p': 9, 's': 18, 'z': 27}
    zhai_map = {"æ±": "1z", "å—": "2z", "è¥¿": "3z", "åŒ—": "4z", "ç™½": "5z", "ç™¼": "6z", "ç™º": "6z", "ä¸­": "7z"}
    hai = zhai_map.get(hai, hai)
    return suit_map[hai[1]] + int(hai[0]) - 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-r", "--run", action="store_true", help="AI ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
    parser.add_argument("-t", "--train", action="store_true", help="AI ã‚’å­¦ç¿’")
    parser.add_argument("-e", "--epochs", type=int, default=30, help="å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°")
    parser.add_argument("-d", "--datafile", default="dahai_data.txt", help="å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«")

    parser.add_argument("--alternate", action="store_true", help="æ•™å¸«ã‚ã‚Šã¨å¼·åŒ–å­¦ç¿’ã‚’äº¤äº’ã«å®Ÿè¡Œ (2å›)")
    parser.add_argument("--alternate20", action="store_true", help="å¼·åŒ–å­¦ç¿’100å±€ + æ•™å¸«ã‚ã‚Š10ep ã‚’20å›äº¤äº’ã«å®Ÿè¡Œ")

    parser.add_argument("--reinforce-loop", action="store_true", help="å¼·åŒ–å­¦ç¿’ã‚’è¤‡æ•°å›é€£ç¶šã§å®Ÿè¡Œ")
    parser.add_argument("--loop-times", type=int, default=10, help="å¼·åŒ–å­¦ç¿’ã®ç¹°ã‚Šè¿”ã—å›æ•°")
    parser.add_argument("--loop-sleep", type=int, default=5, help="å„å¼·åŒ–å­¦ç¿’å¾Œã®ä¼‘æ†©ç§’æ•°")
    parser.add_argument("--lambda-pg", type=float, default=1.0, help="Policy Gradient loss ã®é‡ã¿ Î»")

    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    make_model()
    ckpt = CKPT_DIR / "mahjong_transformer.pt"
    if ckpt.exists():
        _load_ckpt(ckpt)
        print("[âœ…] ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        print("[âš ] ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ï¼‰")

    # é€šå¸¸ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’
    if args.train:
        train_ai(args.datafile, args.epochs)

    # æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆ10,000å±€ï¼‰
    if args.run:
        run_ai()

    # æ•™å¸«ã‚ã‚Šã¨å¼·åŒ–å­¦ç¿’ã®äº¤äº’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ2å›ã ã‘ï¼‰
    if args.alternate:
        train_loop_alternating(
            datafile=args.datafile,
            alt_count=2,
            reinforce_games=100,
            epochs_each=1
        )

    # å¼·åŒ–å­¦ç¿’100å±€ + æ•™å¸«ã‚ã‚Š10ep ã‚’20å›äº¤äº’ã«å®Ÿè¡Œï¼ˆæœ¬å‘½ï¼‰
    if args.alternate20:
        train_loop_alternating(
            datafile=args.datafile,
            alt_count=100,
            reinforce_games=100,
            epochs_each=2
        )

    # å¼·åŒ–å­¦ç¿’ã ã‘ã‚’é€£ç¶šå®Ÿè¡Œ
    if args.reinforce_loop:
        reinforce_loop_multiple(
            times=args.loop_times,
            games_each=100,
            sleep_sec=args.loop_sleep,
            lambda_pg=args.lambda_pg
        )


